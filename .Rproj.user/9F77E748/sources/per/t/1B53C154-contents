---
title: "Temporary_ underlying_structure_IR"
author: "Jordi Tarroch"
date: "10/30/2019"
output: 
  pdf_document:
    latex_engine: xelatex
---
```{r}

##############
#LIBRARIES####
##############
library(readr)
library(skimr)# Beautiful Summarize
library(corrplot)# Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(tidyverse)#select_at
library(factoextra)
library(FactoMineR)
library(psych)
library(PCAmixdata)
library(ggplot2)
library(pls)

##############
# GET DATA####
##############
path <- 'ACPTIUSD.csv'
raw_data <-  read_delim(path, delim = ';')
typeof(raw_data)
dim(raw_data)
# Interest Rate Swap: An interest rate swap is a forward contract in which one stream of future interest payments is
# exchanged for another based on a specified principal amount. Interest rate swaps usually involve the exchange of a fixed
# interest rate for a floating rate, or vice versa, to reduce or increase exposure to fluctuations in interest rates or to obtain 
# a marginally lower interest rate than would have been possible without the swap.


# DEPO: A deposit is a financial term that means money held at a bank.
# A deposit is a transaction involving a transfer of money to another party for safekeeping. However, a deposit can refer to a 
# portion of money used as security or collateral for the delivery of a good.


#####################
# DATA WRANGLING DATA############################################################################################################
#####################
skim(raw_data) 
# DEPO 1M     195 missing (NAs)

# ── Variable type:character ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────
# variable missing complete   n min max empty n_unique
# X1       0      978 978  10  10     0      978
# 
# ── Variable type:numeric ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
# variable missing complete   n mean   sd   p0  p25  p50  p75 p100     hist
# DEPO 12M       0      978 978 5.97 0.43 5    5.78 5.9  6.09 7.81 ▁▂▇▃▁▁▁▁
# DEPO 1M     195      783 978 5.69 0.24 5.31 5.45 5.66 5.88 6.19 ▅▇▃▇▂▅▃▃
# DEPO 3M       0      978 978 5.74 0.24 5.25 5.56 5.69 5.88 6.5  ▁▅▇▇▂▁▂▁
# DEPO 6M       0      978 978 5.81 0.3  5.12 5.63 5.76 5.91 7    ▁▂▇▆▁▁▁▁
# IRS 10Y       0      978 978 6.69 0.55 5.25 6.2  6.67 7.04 8.34 ▁▂▇▇▇▃▁▁
# IRS 2Y       0      978 978 6.12 0.52 4.82 5.85 6.08 6.32 8.22 ▁▂▇▆▂▁▁▁
# IRS 3Y       0      978 978 6.25 0.54 4.86 5.92 6.2  6.47 8.28 ▁▂▇▇▃▁▁▁
# IRS 4Y       0      978 978 6.35 0.55 4.92 5.99 6.3  6.61 8.29 ▁▂▇▇▃▁▁▁
# IRS 5Y       0      978 978 6.43 0.55 5    6.03 6.38 6.71 8.31 ▁▂▇▇▅▁▁▁
# IRS 7Y       0      978 978 6.55 0.55 5.12 6.12 6.51 6.86 8.33 ▁▂▇▇▆▂▁▁

active_data <- raw_data %>% slice(1:949)
skim(active_data) 
suplementary_data <- raw_data %>% slice(950:978)
skim(suplementary_data) 

# CHECK NA's: no NAs to delete.
# delete NA's
active_data_without_nas <- na.omit(active_data)
skim(active_data_without_nas)

# CHECK NON-SENSE DATA: date as we don't care about the number of the day inside the month and the year.
# As our goal is to predict IRS10, we also remove it from our dataset.

# Following the instructions the instructions we are going to work without the 10 years IR that we want to predict later.

# Excluded vars (factor)
vars <- c("X1",names(active_data_without_nas)[11])
# vars <- c("X1")



```

```{r}

#####################################################################################################################################
# OBJETIVO 2: 
# 1. Tiene sentido llevar a cabo, en este caso, un análisis de componentes principales? Para justificarlo, deberá llevar a cabo
# las pruebas que estime oportunas, como, por ejemplo el análisis de la matriz de correlaciones, el del determinante de dicha 
# matriz, la prueba de esfericidad de Bartlett, el KMO o el MSA.
#####################################################################################################################################


##############################
#EXPLORATORY DATA ANALYSIS####################################################################################
##############################
# Why and for what do I need to do a PCA or factor analysis?
# One of the reasons for running a factor analysis is to reduce the large number of
# variables that describe a complex concept to a few interpretable latent variables (=factor).
# In other words, we would like to find a smaller number of interpretable factors that explain the maximum amount variability
# in the data.


############
# PLOT DATA#
############
time <- c(1/12, 3/12, 6/12, 1, 2, 3, 4, 5, 7)

ggplot(data = active_data_without_nas) +
  geom_boxplot(aes(x = time[1],  `DEPO 1M`), col = 9) +
  geom_boxplot(aes(x = time[2], `DEPO 3M`), col = 1 ) + 
  geom_boxplot(aes(x = time[3],  `DEPO 6M`), col = 2) + 
  geom_boxplot(aes(x = time[4], `DEPO 12M`), col = 3) +
  geom_boxplot(aes(x = time[5], `IRS 2Y`), col = 4) + 
  geom_boxplot(aes(x = time[6], `IRS 3Y`), col = 5) +
  geom_boxplot(aes(x = time[7], `IRS 4Y`), col = 6) +
  geom_boxplot(aes(x = time[8], `IRS 5Y`), col = 7) + 
  geom_boxplot(aes(x = time[9], `IRS 7Y`), col = 8) + 
  ylab("Interest Rate %") + xlab("Time")+  ggtitle("From 1 month to 7 years IRs")


#####################
# CORRELATION MATRIX#
#####################
# We see that there is high correlation between all variables in the dataset. This tells us that it makes sense to apply PCA.

# Correlations
ggcorrplot(cor(active_data_without_nas %>% 
               select_at(vars(-vars)), 
             use = "complete.obs"), 
         method = "circle",type = "upper")


ggcorrplot(cor(active_data_without_nas %>% 
                 select_at(vars(-vars)), 
               use = "complete.obs"),
           hc.order = TRUE,
           type = "lower",  lab = TRUE)

# Other Correlations
chart.Correlation(active_data_without_nas %>% 
                    select_at(vars(-vars)),
                  histogram = TRUE, pch = 19)


########################################
# DETERMINANT OF THE CORRELATION MATRIX#
########################################
# https://www.researchgate.net/post/Can_i_use_factor_analysis_if_the_determinant_of_the_correlation_matrix_is_0000
# https://www.quora.com/What-does-the-determinant-of-the-correlation-matrix-represent
# http://statwiki.kolobkreations.com/index.php?title=Exploratory_Factor_Analysis



# Dealing with items that have a high correlation with another item reduces the determinant.
# The determinant of the correlation matrix will be:
# - Equal to 1.0 only if all correlations equal 0.
# - Less than 1 when there are correlations different from 0.

# The determinant is related to the volume of the space occupied by the swarm of data points represented by standard scores
# on the measures involved.
# When the measures are uncorrelated, this space is a sphere with a volume of 1.
# When the measures are correlated, the space occupied becomes an ellipsoid whose volume is less than 1.

# The determinant condition is best checked at the end of an EFA (Exploratory Factor Analysis) rather than the start to check
# if some items needs to be removed because they either have:
# - A high correlation with another item
# - Or they have a low communality. A communality is the extent to which an item correlates with all other items.
# Higher communalities are better. If communalities for a particular variable are low (between 0.0-0.4), then that variable
# may struggle to load significantly on any factor. Low values indicate candidates for removal after you examine the
# pattern matrix.

det(cor(active_data_without_nas %>% 
          select_at(vars(-vars)), 
        use = "complete.obs"))
# As we have a low determinant near to 0, it make sense to apply PCA to reduce dimensions, because there is a high correlation
# between the variables studied.
# [1] Det = 1.483166e-18


#######
# KMO #
#######
# https://www.ibm.com/support/knowledgecenter/en/SSLVMB_24.0.0/spss/base/idh_fact_des.html
# https://www.statisticshowto.datasciencecentral.com/kaiser-meyer-olkin/
# https://en.wikipedia.org/wiki/Partial_correlation
# https://www.ibm.com/support/knowledgecenter/SSLVMB_26.0.0/statistics_casestudies_project_ddita/spss/tutorials/fac_telco_kmo_01.html

# The Kaiser-Meyer-Olkin measure of sampling adequacy (MSA) tests checks how suited your data is for Factor Analysis by measuring:
# - Whether the partial correlations among variables are too small to apply factor analysis.
# - Sampling adequacy for each variable in the model and for the complete model.

# Measure of Sample Adequacy through the factor of Kaiser-Meyer-Olkin factor adequacy
KMO(cor(active_data_without_nas %>% 
          select_at(vars(-vars)), 
        use = "complete.obs"))

# For reference, Kaiser put the following values on the results:
# 0.00 to 0.49 unacceptable.
# 0.50 to 0.59 miserable.
# 0.60 to 0.69 mediocre.
# 0.70 to 0.79 middling.
# 0.80 to 0.89 meritorious.
# 0.90 to 1.00 marvelous.

# So in our case, our variables are meritorious to apply factor analysis. If we see the formula of KMO:
# where:
# R = [rij] is the correlation matrix and
# U = [uij] is the partial covariance matrix. The summation of the partial covariance matrix (measures the degree of
# association between two random variables, with the effect of a set of controlling random variables removed)  is desired to
# be equal to 0. In the case that it is equal to 0, it means that there is no direct relationship between the variables other
# than a factor that relates them. And to apply factor analysis that's exactly what we want, high correlationship
# between them thanks to a factor that we are going to discover.

################################
# Bartlett's Test of Sphericity#
################################
# https://en.wikipedia.org/wiki/Bartlett%27s_test
# https://www.statology.org/a-guide-to-bartletts-test-of-sphericity/
# Bartlett's test of sphericity tests whether the correlation
# matrix is an identity matrix ( n × n square matrix with ones on the main diagonal and zeros elsewhere),
# which would indicate that the factor model is inappropriate.


# The null hypothesis of the test is that the variables are orthogonal, i.e. not correlated.
# So the determinant of the correlation matrix will be equal or near to 1.0, like the the determinant of the identity matrix.
# And the space represented will be a sphere with a volume of 1.

# The alternative hypothesis is that the variables are not orthogonal, i.e. they are correlated enough to where
# the correlation matrix diverges significantly from the identity matrix.
# So the determinant of the correlation matrix will be less than 1.
# And the space occupied becomes an ellipsoid whose volume is less than 1.



# This test is often performed before we use a data reduction technique such as principal component analysis or factor
# analysis to verify that a data reduction technique can actually compress the data in a meaningful way. 
p_value <- cortest.bartlett(cor(active_data_without_nas %>% 
                       select_at(vars(-vars))), n = NULL,diag=TRUE)$p.value
p_value
# Small values (less than 0.05) of the significance level indicate that a factor analysis may be useful with your data.

# In our case the p-value is equal to 0, so the factor analysis will be useful with our data.

###############################################
# CONCLUSIONS OF THE DATA EXPLORATORY ANALYSIS#
###############################################
# After doing all these tests we can apply the Principal Component Analysis with confidence.
###################################END OF DATA EXPLORATORY ANALYSIS#####################################################




#####################################################################################################################################
#2. ¿Cuántos componentes permitirían explicar, adecuadamente, la estructura subycente de los tipos de interés aquí analizados? 
# Justifique su respuesta empleando, por ejemplo, las pruebas de la varianza explicada o del gráfico de sedimentación;
#####################################################################################################################################
# After doing the principal component analysis and seeing:
#   - Individuals factor map and Variables factor map(PCA): where Dim1 and Dim2 explain 98.87% of the variance.
#   - cos2: the squared loadings for variables, whose values explain us that Dim2 explains much better short term interest rates
#     and Dim1 explains much better longer term interest rates. Once we get to Dim3 and further dimensions, most of the
#     squared loadings get values of the e^-02 scale (really small values).
#   - Scree Plot: where Dim1 and Dim2 explain 98.87% of the variance, and once we get to dimension 3, the slope almost
#     doesn't change.
#   - Eigenvalues: showing us the variance percentage and cumulative variance percentage, which doesn't almost change once we get to the
#     third dimension.

# We conclude that we can explain the structure below the interest rates studied just by using first two principal components.


# Here we can see the analysis done:
###############################
# PRINCIPAL COMPONENT ANALYSIS##############################################################################################
###############################
# https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
# https://www.youtube.com/watch?v=2fCBE7DWgd0
# http://www.sthda.com/english/wiki/print.php?id=204
# First principal component is a linear combination of original predictor variables which captures the maximum 
# variance in the data set.
# Second principal component (Z²) is also a linear combination of original predictors which captures part of the remaining
# variance in the data set and is uncorrelated with Z¹.

#######################################################
# Individuals factor map and Variables factor map(PCA)#
#######################################################


pca = PCA(active_data_without_nas %>%
            select_at(vars(-vars)), graph = T)
# As we can see in Individuals factor map, the directions of Dim1 and Dim2 are clearly orthogonal and explain
# 98.87% of the variance:
# - 81.7% of the variance can be explained in Dim1.
# - 17.58% of the variance can be explained in Dim2.

# If the first three factors together explain most of the variability in the original 9 variables, then those
# factors are clearly a good, simpler substitute for all 10 variables.  We can drop the rest without losing much of
# the original variability.
# But if it takes 7 factors to explain most of the variance in those 10 variables, we might as well just use the original 10.



#######################
# Loadings/Coordinates#
#######################
# The correlation between a variable and a PC is called loading. The variables can be plotted as points in the component
# space using their loadings as coordinates. Here we can see the coordinates that correspond with the Variables factor map (PCA).
pca$var$coord

######################################################################
# cos2: quality of the representation for variables on the factor map#
######################################################################
# The squared loadings for variables are called cos2 ( = cor * cor = coord * coord).
# The sum of the cos2 for variables on the principal components is equal to one.
# The cos2 values are used to estimate the quality of the representation
# The closer a variable is to the circle of correlations, the better its representation on the factor map 
# (and the more important it is to interpret these components)
# Variables that are closed to the center of the plot are less important for the first components.
pca$var$cos2
# Here we start seeing how:
#   -Dim2 explains much better short term interest rates.
#   -Dim1 explains much better longer term interest rates.
#   -Once we get to Dim3 and further dimensions, most of the squared loadings get values of the e^-02 scale (really small values).



#############
# Scree Plot#
#############
fviz_eig(pca, addlabels = TRUE, hjust = -0.3) + 
  labs(title = "Scree plot", x = "Dimensions", y = "% Variance explained") +
  theme_minimal()
# Dimensions are ordered by eigenvalues or variation explanatory power.
# The point where the slope of the curve is clearly leveling off (the “elbow) indicates the number of factors
# that should be generated by the analysis.
# Dim1 and Dim2 explain 98.87% of the variance, and once we get to dimension 3, the slope doesn't even change.

################
# Contributions#
################
fviz_contrib(pca, choice="var", axes = 1 )+
  labs(title = "Contributions to Dim 1")
# If the contribution of the variables were uniform, the expected value would be 1/length(variables) = 1/9 = 11.1%. So this is
# the expected average contribution that we can use as a threshold.
# Using  that as a threshold, here we can see the highest contributions to Dim1: DEPO12M, IRS2Y, IRS3Y, IRS4Y, IRS5Y,
# DEPO6M, IRS7Y.


fviz_contrib(pca, choice="var", axes = 2 )+
  labs(title = "Contributions to Dim 2")
# Using  that as a threshold, here we can see the highest contributions to Dim2: DEPO1M, DEPO3M (short term interest rates).


##############
# Eigenvalues#
##############
get_eig(pca)
# Eigenvalues correspond to the amount of the variation explained by each principal component (PC) and it is calculated
# from det(covariance matrix-lambda*Identity_Matrix) = 0 giving as a result an equation that can be solved to find
# lambdas (eigenvalues).

# Using the definition of the eigenvector we can find the eigenvectors, from the expression:
# covariance_matrix*ei = lambda_i*ei, giving as a result an eigenvector for each eigenvalue. We want an unit length
# eigenvector ||ei|| = 1.

# Once we have the eigenvectors, they are the principal components we were looking for our PCA, and we can see the
# variance percentage we are interested in on each dimension.





#####################################################################################################################################
# 3. Finalmente, ¿tiene sentido llevar a cabo una rotación de las variables subyacentes? Para responder, lleva a cabo una rotación
# Varimax, por ejemplo.
######################################################################################################################################
# https://www.youtube.com/watch?v=oZ2nfIPdvjY
# https://www.r-bloggers.com/naive-principal-component-analysis-using-r/
# https://methods.sagepub.com/reference/the-sage-encyclopedia-of-communication-research-methods/i5417.xml
# https://www.theanalysisfactor.com/rotations-factor-analysis/
# The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, 
# will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are
# shuffled).

# The go-to rotation method is the orthogonal, or ‘varimax’, a statistical technique used at one level of factor analysis as 
# an attempt to clarify the relationship among factors.

# Explained variance is captured better this way. The adjustment, or rotation, is intended to maximize the variance shared 
# among items. By maximizing the shared variance, results more discretely represent how data correlate with each principal
# component. To maximize the variance generally means to increase the squared correlation of items related to one factor, 
# while decreasing the correlation on any other factor. In other words, the varimax rotation simplifies the loadings 
# of items by removing the middle ground and more specifically identifying the factor upon which data load.

# Generally, the process involves adjusting the coordinates of data that result from a principal components 
# analysis.

###################
# Varimax method 1#
###################
# my_pca <- prcomp(active_data_without_nas %>% 
#                    select_at(vars(-vars)), center = T, scale = T )
# my_pca$rotation
# my_var <- varimax(my_pca$rotation)
# my_var
# my_var$loadings
# pca$var$coord
# pca$var$cos2


###################
# Varimax method 2#
###################
# Now with varimax rotation, Kaiser-normalized 
# by default:
pc2 = psych::principal(cor(active_data_without_nas %>% 
                             select_at(vars(-vars))), nfactors=2, 
                       rotate = "varimax", scores = TRUE)
pc2
# In pc2 we see how each variable correlates with the rotated components. Which reveals how variables load on each component,
# or in other words, to which component a variable belongs. 

pc2$loadings
pca$var$coord
# After doing the rotation, comparing the loadings (the correlation between a variable and a PC) of Dim1 and Dim2 of 
# the inicial pca and the second pca2, we keep seeing that Dim1 and Dim2 keep explaining the same structure below the
# interest rates studied. So to the result of our PC1 and PC2 is robust enough even when we rotate the variables.



# Loadings of first pca:
#           RC1   RC2
# DEPO 1M        0.986
# DEPO 3M  0.358 0.929
# DEPO 6M  0.667 0.736
# DEPO 12M 0.866 0.482
# IRS 2Y   0.947 0.309
# IRS 3Y   0.966 0.257
# IRS 4Y   0.974 0.224
# IRS 5Y   0.977 0.199
# IRS 7Y   0.978 0.160

# Loadings of second pca2:
#           Dim.1       Dim.2       
# DEPO 1M  0.5175461  0.84046147  
# DEPO 3M  0.7575275  0.64628455 
# DEPO 6M  0.9367010  0.32900245 
# DEPO 12M 0.9906092  0.01087568 
# IRS 2Y   0.9802660 -0.17988615 
# IRS 3Y   0.9712977 -0.23428810 
# IRS 4Y   0.9626448 -0.26729063  
# IRS 5Y   0.9534477 -0.29112569  
# IRS 7Y   0.9356304 -0.32569222


# Healthcheck
pc2$residual
pc2$fit
pc2$communality

# We would want:
# - Less than half of residuals with absolute values > 0.05
# - Model fit > .9
# - All communalities > .7





#####################################################################################################################################
# OBJETIVO 1: 
#   OBSERVACIONES:  950 a 978 (llamadas observaciones SUPLEMENTARIAS) --> suplementary_data
#   PREDICCIÓN:  predecir el valor del bono a 10 años (IRS.10Y, variable suplementaria)--> LAST COLUMN OF THE DATASET
#####################################################################################################################################
raw_data_without_nas <- na.omit(raw_data)
vars <- c("X1")
data <- raw_data_without_nas %>%select_at(vars(-vars))

###########
# METHOD 1#
###########
# split data into 2 parts for pca training (75%) and prediction (25%)
# set.seed(1)
# samp <- sample(nrow(data), nrow(data)*0.75)
# data.train <- data[samp,]
# data.valid <- data[-samp,]
# 
# #PCA with scaling
# prin_comp <- prcomp(data.train, scale. = T)
# 
# train.data <- data.frame(IRS_10 = data.train$`IRS 10Y`, prin_comp$x)
# train.data
# #First 10 PCAs
# train.data <- train.data[ , 1:10]
# 
# #Train decision tree on IRS_10 again 10 components of train data
# library(rpart)
# rpart.model <- rpart(IRS_10 ~ . , data = train.data, method = "anova")
# 
# #Transform test into PCA by using princomp from train to pca.test. newdata looks for which variable to predict
# test.data <- predict(prin_comp, newdata = data.valid)
# 
# #predicited PCA with test one hot encoded dataset
# test.data <- as.data.frame(test.data)
# 
# #First 10 components
# test.data <- test.data[,1:10]
# 
# #Prediction on test data. Predicts the IRS10based on the components of the test data
# rpart.prediction <- predict(rpart.model, test.data)
# rpart.prediction


###########
# METHOD 2#
###########
#####################
# TRAIN-TEST DATASET#
#####################

training <- active_data
test     <- suplementary_data
# Remember that we have NAs in DEPO 1M column. We are going to ignore this column to crete the model. 


########
# MODEL#
########

# Principal components regression (PCR) is a regression technique based on principal 
# component analysis (PCA).
# ncomp: numeric; the PCR components for which predictions are sought. If ncomp = c, predictions
# for components 1:c are produced.
pcr.model <- pcr(formula = `IRS 10Y` ~ . -`X1`-`DEPO 1M`,
                  data = training,
                  ncomp = 2)

pcr.model$coefficients

predictions <- predict(pcr.model, newdata = test, ncomp = 2)
######
# MSE#
######
mse <- mean((predictions - test$`IRS 10Y`)^2)
mse
```
